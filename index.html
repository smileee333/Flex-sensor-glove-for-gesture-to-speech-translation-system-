<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.14.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Gesture translator: Gesture to Speech Translator</title>
<link rel="icon" href="Flex_logo.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript" src="darkmode_toggle.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="Flex_logo.png"/></td>
  <td id="projectalign">
   <div id="projectname">Gesture translator<span id="projectnumber">&#160;0.0.1</span>
   </div>
   <div id="projectbrief">to translate sign language gestures into speech signal</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.14.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('index.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Gesture to Speech Translator </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1 class="doxsection"><a class="anchor" id="Intro"></a>
Introduction</h1>
<p>Speech impaired individuals often use sign language to communicate with other individuals around them. Sign language as visual-gestural manual language is well established throughout the globe and has been proven to be an effective method for communication for individuals with speech impairment. However it also have it's own set of challenges and pre-requisites. One of the primary pre-requisite is that, individual with whom communication is required to made, must also have a good understanding of sign language used by the speech impairment. Interpreter must have knowledge about the meaning of signs made by the communicator to understand their expression. This pre-requisite requirement becomes a key challenge in the applicability of sign language. The aim of this project is to remove this essential pre-requisite requirement by providing a cost effective and easy to use solution, to make sign language a useful tool to counter the difficulties faced by individuals due to speech impairment.</p>
<h1 class="doxsection"><a class="anchor" id="Assume"></a>
Assumptions</h1>
<p>Expression to be expressed through gestures can be considered as a group of sentences, one after the other. Further each sentence is considered to be collection of specific words or phrases placed in a certain order. Thus whole expression to be conveyed by communicator can be considered as a group of words or phrases, put in a certain order, so that communicator is able to express his idea to the interpreter. Speech impaired individuals uses the gestures defined in sign language to convey each word/phrase and then they change the gesture to convey next word/phrase. Each gesture made by communicator can be thought as a specific static position or movement of figures and hands at certain angles. Thus hand movements and figure movements are considered to be two sources of gesture which are required to be analyzed to interpret the word/phrase out of it. Following assumptions are made on top of it.</p><ul>
<li>For sake of simplicity, finger movements are considered to be only source of gesture in this project.</li>
<li>Flex sensors are used to capture the movement of fingers.</li>
<li>Over a period of time, data captured by flex sensors may include drift and other related errors. To compensate for errors, only two bent levels are considered in data sensed by flex sensors.</li>
</ul>
<h1 class="doxsection"><a class="anchor" id="Problems"></a>
Problem statement</h1>
<p>As per assumption, whole expression required to be conveyed is considered to be collection of specific words/phrases put in certain order and each word/phrase is expressed by certain specific gesture. Thus a gesture corresponding to a word/phrase is required to be interpreted and translated to speech signal. This shall be done over all the acquired words/phrases so that complete expression is conveyed to interpreter without the requirement of understanding of sign language. Thus an intermediate gesture to speech translator is required to remove the primary hurdle in applicability of sign language.</p>
<h1 class="doxsection"><a class="anchor" id="Ket_Principle"></a>
Key principle</h1>
<p>The proposed gesture-to-speech translation system is predicated on a lookup-based architecture. Upon gesture recognition, the system retrieves and plays the appropriate pre-recorded audio file from its repository. The implementation maintains a bijective relationship between the gesture vocabulary and stored audio samples.</p>
<h1 class="doxsection"><a class="anchor" id="Requirements"></a>
Requirement Specification</h1>
<p>As per assumptions and problem statement description following set of inputs and outputs are evident</p><ul>
<li>inputs: flex sensors for each figure of both hands, a start button to start the gesture capturing.</li>
<li>outputs: a speech signal for each gesture corresponding to word/phrase. Apart from primary inputs and outputs, there should be a provision of display panel for input/output observability. Apart from letting the translator translate each word/phrase specific captured gestures, there should also be a provision to review all the words/phrases captured by translator along with the provision to delete them before letting the translator, translate them to speech signal. There will be a set of words/phrases along with the corresponding gestures, pre-defined in the translator, but translator's firmware architecture should be scalable to scale to more set of words/phrase support along with hand-movement sensors support. Each recognized sampled gestures from flex sensors should have hamming distance greater then certain threshold, so that these gestures becomes easily distinguishable from one another.<br  />
 <br  />
 Following diagram gives the visual representation or final solution: <div class="image">
<img src="/images/hand.png" alt="gesture translator" width="1000px"/>
<div class="caption">
Fig. Conceptualization of Gesture to speech translator</div></div>
</li>
</ul>
<h1 class="doxsection"><a class="anchor" id="Challanges"></a>
Key Challanges</h1>
<h2 class="doxsection"><a class="anchor" id="Challange_1"></a>
Gesture Distinguishability</h2>
<p>Due to a minimum hamming distance requirement, all the possible gestures from flex sensors output can't be directly mapped to words/phrases defined in sign language. Gestures defined in sign language are not subjected to the condition of minimum hamming distance. Thus we can't have a common list of all the gestures put together to be analyzed, otherwise their may be chances of gesture overlapping resulting in miscommunication.<br  />
<b>Solution</b> All the sign language gestures are required to be segregated into different groups such that within each group, only those sign language gestures are kept together, which are having bare minimum required hamming distance.Further gestures are required to be analyzed within each group separately such that only one gesture group is considered to be active at a time. Thus following Finite state machine can be considered for gesture transitioning such that each state of FSM represents a gesture: </p><div class="image">
<img src="/images/Gesture_FSM.png" alt="Gesture finite state machine" width="1000px"/>
<div class="caption">
Fig. Gesture Finite State Machine</div></div>
<h2 class="doxsection"><a class="anchor" id="Challenge_2"></a>
Wearability, low-power, cost effective and memory constrain</h2>
<p>Provided solution should be cost effective, low power, wearable as well as implementable in a large variety of gesture to speech translation requirement. It indirectly puts the constraints of translator being memory constrained device along with the requirement of support for large number of words/phrases specific gestures. Hamming distance puts a critical upper bound over number of supportable gestures at a time. Also few extra control gestures and generic gestures are also required to be defined on the top of supported sign language gestures, for translator management purposes.<br  />
<b>Solution</b> With such a memory constrained device, in addition to solution of only one gesture group being active at a time, gestures should also be multiplexed such that within a cetain active gesture group, it should mean different and in another time instance when another group is active, it should mean different. Follwing diagram illustrates the gesture bank architecture described here: </p><div class="image">
<img src="/images/Gesture_bank.png" alt="Gesture bank" width="1000px"/>
<div class="caption">
Fig. Gesture Bank Concept</div></div>
<p><br  />
 Due to low power constraints, in order to save power, translator's operation should remain off when no sampling of gesture and translation to speech is going on.</p>
<h2 class="doxsection"><a class="anchor" id="Challenge_3"></a>
Scalability and design re-use</h2>
<p>To scalability requirement and design re-use, the existing architecture to extend the support for more sign language or translator management gestures, a template based architecture should be used in conjunction with object oriented. Following diagram highlights the solution space objects along with the inter-object communication between them: </p><div class="image">
<img src="Firmware_architecture.png" alt="Firmware architecture" width="1000px"/>
<div class="caption">
Fig. Solution space objects and interconnection between them.</div></div>
<h1 class="doxsection"><a class="anchor" id="Prototyping"></a>
Design prototyping</h1>
<div class="image">
<img src="/images/flex_schematic.png" alt="prototype schematic" width="1000px"/>
<div class="caption">
Fig. Prototype design schmatic</div></div>
 </div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
</div><!-- doc-content -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.14.0 </li>
  </ul>
</div>
</body>
</html>
